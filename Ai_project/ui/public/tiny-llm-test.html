<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>TinyLlama Test</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Arial, sans-serif;
            line-height: 1.5;
            color: #333;
            max-width: 800px;
            margin: 0 auto;
            padding: 20px;
        }
        
        h1, h2 {
            color: #2563eb;
        }
        
        .card {
            border: 1px solid #e5e7eb;
            border-radius: 8px;
            padding: 20px;
            margin-bottom: 20px;
            background-color: white;
            box-shadow: 0 1px 3px rgba(0, 0, 0, 0.1);
        }
        
        button {
            background-color: #2563eb;
            color: white;
            border: none;
            padding: 8px 16px;
            border-radius: 4px;
            cursor: pointer;
            font-weight: 500;
            margin-top: 10px;
        }
        
        button:disabled {
            background-color: #93c5fd;
            cursor: not-allowed;
        }
        
        button:hover:not(:disabled) {
            background-color: #1d4ed8;
        }
        
        .status {
            margin-top: 10px;
            padding: 10px;
            border-radius: 4px;
        }
        
        .success {
            background-color: #ecfdf5;
            color: #047857;
        }
        
        .error {
            background-color: #fef2f2;
            color: #b91c1c;
        }
        
        .loading {
            background-color: #eff6ff;
            color: #1e40af;
        }
        
        .prompt-area {
            margin-top: 20px;
        }
        
        textarea {
            width: 100%;
            padding: 8px;
            border: 1px solid #d1d5db;
            border-radius: 4px;
            margin-top: 5px;
            margin-bottom: 10px;
            min-height: 60px;
        }
        
        .result {
            margin-top: 20px;
            padding: 15px;
            background-color: #f8fafc;
            border-radius: 4px;
            border: 1px solid #e2e8f0;
            white-space: pre-wrap;
        }
        
        .note {
            font-size: 0.9em;
            color: #64748b;
            font-style: italic;
        }
        
        .links {
            margin-top: 30px;
            display: flex;
            gap: 10px;
        }
        
        .links a {
            color: #2563eb;
            text-decoration: none;
        }
        
        .links a:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <h1>TinyLlama Test</h1>
    <p>This page tests a smaller language model that requires fewer resources.</p>
    
    <div class="card">
        <h2>Model Initialization</h2>
        <p>TinyLlama is a 1.1B parameter model, much smaller than the 2.7B Phi-2 model.</p>
        <button id="load-btn">Load TinyLlama Model</button>
        <div id="status" class="status"></div>
    </div>
    
    <div class="card" id="chat-card" style="display: none;">
        <h2>Generate Text</h2>
        <div class="prompt-area">
            <label for="prompt">Enter your prompt:</label>
            <textarea id="prompt">Create a professional interview question for a Frontend Developer position that tests both technical knowledge and problem-solving skills.</textarea>
            <button id="generate-btn">Generate Response</button>
        </div>
        <div id="result" class="result" style="display: none;"></div>
    </div>
    
    <div class="card">
        <h2>About TinyLlama</h2>
        <p>TinyLlama is a small open-source language model with 1.1B parameters, created through a condensed training process inspired by LLaMA. It's designed to be computationally efficient while maintaining reasonable performance for basic tasks.</p>
        <p class="note">Note: This model has more limited capabilities than larger models like Phi-2, but requires significantly less memory and processing power.</p>
    </div>
    
    <div class="links">
        <a href="/llm-test-link.html">‚Üê Back to Test Links</a> | 
        <a href="/local-fallback.html">Local Fallback Options</a> | 
        <a href="/">Home</a>
    </div>
    
    <script>
        // Model variables
        let modelLoaded = false;
        let generating = false;
        
        // UI elements
        const loadBtn = document.getElementById('load-btn');
        const statusEl = document.getElementById('status');
        const chatCard = document.getElementById('chat-card');
        const promptInput = document.getElementById('prompt');
        const generateBtn = document.getElementById('generate-btn');
        const resultEl = document.getElementById('result');
        
        // Load script dynamically
        function loadScript(url) {
            return new Promise((resolve, reject) => {
                const script = document.createElement('script');
                script.src = url;
                script.onload = resolve;
                script.onerror = reject;
                document.head.appendChild(script);
            });
        }
        
        // Check if WebLLM is loaded
        function isWebLLMLoaded() {
            return typeof WebLLM !== 'undefined';
        }
        
        // Load TinyLlama model
        loadBtn.addEventListener('click', async () => {
            loadBtn.disabled = true;
            statusEl.className = 'status loading';
            statusEl.textContent = 'Loading WebLLM library...';
            
            try {
                // Try to load WebLLM library first
                if (!isWebLLMLoaded()) {
                    try {
                        await loadScript('https://cdn.jsdelivr.net/npm/@mlc-ai/web-llm/dist/web-llm.umd.min.js');
                    } catch (err) {
                        try {
                            await loadScript('https://unpkg.com/@mlc-ai/web-llm/dist/web-llm.umd.min.js');
                        } catch (err2) {
                            throw new Error('Failed to load WebLLM library. Please try the local fallback option.');
                        }
                    }
                }
                
                if (!isWebLLMLoaded()) {
                    throw new Error('WebLLM library failed to initialize properly.');
                }
                
                statusEl.textContent = 'Initializing TinyLlama model...';
                
                // Create and initialize model
                window.llm = new WebLLM();
                
                // Set progress callback
                window.llm.setProgressCallback((current, total) => {
                    const percent = Math.round((current / total) * 100);
                    statusEl.textContent = `Loading model: ${percent}% (${current}/${total})`;
                });
                
                // Load the smaller TinyLlama model
                await window.llm.reload({
                    model: 'TinyLlama-1.1B-Chat-v1.0-q4f16_1',
                    modelUrl: 'https://huggingface.co/mlc-ai/tinyllama-1.1b-chat-v1.0-q4f16_1/resolve/main/'
                });
                
                // Update UI
                statusEl.className = 'status success';
                statusEl.textContent = 'TinyLlama model loaded successfully!';
                loadBtn.textContent = 'Model Loaded';
                chatCard.style.display = 'block';
                modelLoaded = true;
                
            } catch (error) {
                console.error('Error loading model:', error);
                statusEl.className = 'status error';
                statusEl.textContent = `Error: ${error.message}`;
                loadBtn.disabled = false;
                loadBtn.textContent = 'Try Again';
            }
        });
        
        // Generate text
        generateBtn.addEventListener('click', async () => {
            if (!modelLoaded || generating) return;
            
            const prompt = promptInput.value.trim();
            if (!prompt) return;
            
            generating = true;
            generateBtn.disabled = true;
            generateBtn.textContent = 'Generating...';
            resultEl.textContent = '';
            resultEl.style.display = 'none';
            
            try {
                const response = await window.llm.generate(prompt, {
                    temperature: 0.7,
                    max_gen_len: 256
                });
                
                resultEl.textContent = response.output;
                resultEl.style.display = 'block';
                
            } catch (error) {
                console.error('Generation error:', error);
                resultEl.textContent = `Error: ${error.message}`;
                resultEl.style.display = 'block';
                resultEl.className = 'result error';
            } finally {
                generating = false;
                generateBtn.disabled = false;
                generateBtn.textContent = 'Generate Response';
            }
        });
    </script>
</body>
</html> 